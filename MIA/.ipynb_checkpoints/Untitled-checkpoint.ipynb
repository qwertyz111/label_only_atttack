{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7f21a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "======================AdversaryOne AUC of Loss, Entropy, Maximum respectively cluster:3000 ===================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 552\u001b[0m\n\u001b[1;32m    549\u001b[0m             Decision_Radius(args)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 552\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 540\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# attack\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mAdversaryOne\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:    \n\u001b[1;32m    542\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/CIFAR100\u001b[39m\u001b[38;5;124m'\u001b[39m  \n",
      "Cell \u001b[0;32mIn[2], line 364\u001b[0m, in \u001b[0;36mAdversaryOne\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39madvOne_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    363\u001b[0m     logx\u001b[38;5;241m.\u001b[39mmsg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m======================AdversaryOne AUC of Loss, Entropy, Maximum respectively cluster:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ===================\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cluster))\n\u001b[0;32m--> 364\u001b[0m     AUC_Loss, AUC_Entropy, AUC_Maximum \u001b[38;5;241m=\u001b[39m \u001b[43mAdversaryOne_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshadowmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUC_Loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUC_Entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAUC_Maximum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m#print('AUC_Loss: ', AUC_Loss)\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m#print('AUC_Entropy: ', AUC_Entropy)\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m#print('AUC_Maximum: ', AUC_Maximum)\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39madvOne_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss_visual\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/1/Decision-based-MIA-main2/attack.py:311\u001b[0m, in \u001b[0;36mAdversaryOne_evaluation\u001b[0;34m(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\u001b[0m\n\u001b[1;32m    308\u001b[0m         dqn\u001b[38;5;241m.\u001b[39mstore_transition(state_list[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m20\u001b[39m],action_list[j],reward_list[j],state_list[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m21\u001b[39m])\n\u001b[1;32m    309\u001b[0m         state_list\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m         \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         j\u001b[38;5;241m=\u001b[39mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of members and no-members : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_mem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/1/Decision-based-MIA-main2/attack.py:103\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 输入32个评估值和32个目标值，使用均方损失函数\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()                                      \u001b[38;5;66;03m# 清空上一步的残余更新参数值\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                                                 \u001b[38;5;66;03m# 误差反向传播, 计算参数更新值\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/Dy/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Dy/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from runx.logx import logx\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from models import ResNet18\n",
    "from classifier import CNN\n",
    "from utils import load_dataset, init_func, Rand_Augment\n",
    "from deeplearning import train_target_model, test_target_model, train_shadow_model, test_shadow_model\n",
    "from attack import AdversaryOne_Feature, AdversaryOne_evaluation, AdversaryTwo_HopSkipJump, AdversaryTwo_QEBA,AdversaryTwo_SaltandPepperNoise\n",
    "from cert_radius.certify import certify\n",
    "\n",
    "\n",
    "action = -1\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义Net类 (定义网络)\n",
    "\n",
    "N_ACTIONS = 2               \n",
    "N_STATES = 20\n",
    "\n",
    "BATCH_SIZE = 32                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.99                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率\n",
    "MEMORY_CAPACITY = 2000                          # 记忆库容量\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                                                         # 定义Net的一系列属性\n",
    "        # nn.Module的子类函数必须在构造函数中执行父类的构造函数\n",
    "        super(Net, self).__init__()                                             # 等价与nn.Module.__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)                                      # 设置第一个全连接层(输入层到隐藏层): 状态数个神经元到50个神经元\n",
    "        self.fc1.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)                                     # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元\n",
    "        self.out.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "\n",
    "    def forward(self, x):                                                       # 定义forward函数 (x为状态)\n",
    "        x = F.relu(self.fc1(x))                                                 # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值\n",
    "        actions_value = self.out(x)                                             # 连接隐藏层到输出层，获得最终的输出值 (即动作值)\n",
    "        return actions_value \n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):                                                         # 定义DQN的一系列属性\n",
    "        self.eval_net, self.target_net = Net(), Net()                           # 利用Net创建两个神经网络:评估网络和目标网络\n",
    "        self.learn_step_counter = 0                                             # for target updating\n",
    "        self.memory_counter = 0                                                 # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 +2 ))             # 初始化记忆库，一行代表一个transition\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # 使用Adam优化器(输入为评估网络的参数和学习率)\n",
    "        self.loss_func = nn.MSELoss()                                           # 使用均方损失函数 (loss(xi, yi)=(xi-yi)^2)\n",
    "\n",
    "    def choose_action(self, x):                                                 # 定义动作选择函数 (x为状态)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "        if np.random.uniform() < EPSILON:                                       # 生成一个在[0, 1)内的随机数，如果小于EPSILON，选择最优动作\n",
    "            actions_value = self.eval_net.forward(x)                            # 通过对评估网络输入状态x，前向传播获得动作值\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "            action = action[0]                                                  # 输出action的第一个数\n",
    "        else:                                                                   # 随机选择动作\n",
    "            action = np.random.randint(0, N_ACTIONS)                            # 这里action随机等于0或1 (N_ACTIONS = 2)\n",
    "        return action                                                           # 返回选择的动作 (0或1)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):                                    # 定义记忆存储函数 (这里输入为一个transition)\n",
    "        transition = np.hstack((s, [a, r], s_))                                 # 在水平方向上拼接数组\n",
    "        # 如果记忆库满了，便覆盖旧的数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           # 获取transition要置入的行数\n",
    "        self.memory[index, :] = transition                                      # 置入transition\n",
    "        self.memory_counter += 1                                                # memory_counter自加1\n",
    "\n",
    "    def learn(self):                                                            # 定义学习函数(记忆库已满后便开始学习)\n",
    "        # 目标网络参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 一开始触发，然后每100步触发\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         # 将评估网络的参数赋给目标网络\n",
    "        self.learn_step_counter += 1                                            # 学习步数自加1\n",
    "\n",
    "        # 抽取记忆库中的批数据\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            # 在[0, 2000)内随机抽取32个数，可能会重复\n",
    "        b_memory = self.memory[sample_index, :]                                 # 抽取32个索引对应的32个transition，存入b_memory\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        # 将32个s抽出，转为32-bit floating point形式，并存储到b_s中，b_s为32行4列\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        # 将32个a抽出，转为64-bit integer (signed)形式，并存储到b_a中 (之所以为LongTensor类型，是为了方便后面torch.gather的使用)，b_a为32行1列\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        # 将32个r抽出，转为32-bit floating point形式，并存储到b_s中，b_r为32行1列\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        # 将32个s_抽出，转为32-bit floating point形式，并存储到b_s中，b_s_为32行4列\n",
    "\n",
    "        # 获取32个transition的评估值和目标值，并利用损失函数和优化器进行评估网络参数更新\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "        # eval_net(b_s)通过评估网络输出32行每个b_s对应的一系列动作值，然后.gather(1, b_a)代表对每行对应索引b_a的Q值提取进行聚合\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        # q_next不进行反向传递误差，所以detach；q_next表示通过目标网络输出32行每个b_s_对应的一系列动作值\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        # q_next.max(1)[0]表示只返回每一行的最大值，不返回索引(长度为32的一维张量)；.view()表示把前面所得到的一维张量变成(BATCH_SIZE, 1)的形状；最终通过公式得到目标值\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        # 输入32个评估值和32个目标值，使用均方损失函数\n",
    "        self.optimizer.zero_grad()                                      # 清空上一步的残余更新参数值\n",
    "        loss.backward()                                                 # 误差反向传播, 计算参数更新值\n",
    "        self.optimizer.step()\n",
    "        \n",
    "def Train_Target_Model(args):\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    for idx, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache() \n",
    "        logx.initialize(logdir=args.logdir + '/target/' + str(cluster), coolname=False, tensorboard=False)\n",
    "        train_loader, test_loader = load_dataset(args, dataset, cluster, mode=args.mode_type)\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        targetmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        optimizer = optim.Adam(targetmodel.parameters(), lr=args.lr)\n",
    "        logx.msg('======================Train_Target_Model {} ===================='.format(cluster))\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train_target_model(args, targetmodel, train_loader, optimizer, epoch)\n",
    "            test_target_model(args, targetmodel, test_loader, epoch, save=True)\n",
    "\n",
    "\n",
    "def Train_Shadow_Model(args):\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    save=True\n",
    "    for idx, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_loader = load_dataset(args, dataset, cluster, mode=args.mode_type)\n",
    "        #train_loader, test_loader = load_dataset(args, dataset, cluster, mode=args.mode_type)\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        shadowmodel = CNN('CNN7', dataset)\n",
    "        \n",
    "        targetmodel.apply(init_func)\n",
    "        shadowmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "        \n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        dqn=DQN()\n",
    "        state_list=[]\n",
    "        action_list=[]\n",
    "        reward_list=[]\n",
    "        for i in range(20):\n",
    "            state_list.append(1)\n",
    "            \n",
    "        logx.initialize(logdir=args.logdir + '/shadow/'+ str(cluster), coolname=False, tensorboard=False)\n",
    "        optimizer = optim.Adam(shadowmodel.parameters(), lr=args.lr)\n",
    "        logx.msg('======================Train_Shadow_Model {} ===================='.format(cluster))\n",
    "        \n",
    "        j=0\n",
    "        accuracy1=[]\n",
    "        num_pred=0\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            \n",
    "            if j<=20:\n",
    "                action_list.append(1)\n",
    "            else:\n",
    "                action_list.append(dqn.choose_action(state_list[j]))\n",
    "            #train_shadow_model(args, targetmodel, shadowmodel, train_loader, optimizer, epoch)\n",
    "\n",
    "            targetmodel.eval()\n",
    "            shadowmodel.train()\n",
    "                               \n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = data.cuda()\n",
    "                output = targetmodel(data)\n",
    "                _, target = output.max(1)\n",
    "                optimizer.zero_grad()\n",
    "                output = shadowmodel(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch_idx % args.log_interval == 0:\n",
    "                    logx.msg('ShadowModel Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "            #test_shadow_model(args, targetmodel, shadowmodel, train_loader, epoch, save=True)\n",
    "            targetmodel.eval()\n",
    "            shadowmodel.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                    data = data.cuda()\n",
    "                    output = targetmodel(data)\n",
    "                    _, target = output.max(1)\n",
    "\n",
    "                    output = shadowmodel(data)\n",
    "                    test_loss += F.cross_entropy(output, target).item()\n",
    "                    pred = output.max(1, keepdim=True)[1]\n",
    "                    #print(pred)\n",
    "                     \n",
    "                    reference_tensor = pred\n",
    "                    \n",
    "                    if action_list[j]==1:\n",
    "                         # generate random array of size [128, 1] with integer values between 0 and 9\n",
    "                        random_array = np.random.randint(0, 10, size=(128, 1))\n",
    "                        #random_int = random.randint(0, 10)\n",
    "                        # convert the array to a PyTorch tensor\n",
    "                        pred = torch.from_numpy(random_array).cuda()\n",
    "                        pred = pred[:reference_tensor.shape[0]]\n",
    "                        for k in range(len(target)):\n",
    "                            target = torch.cat([target[:k], target[k+1:]], dim=0)\n",
    "                            pred=torch.cat([pred[:k], pred[k+1:]], dim=0)\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                    accuracy1.append(correct/len(pred))\n",
    "                    num_pred+=len(pred)\n",
    "            test_loss /= len(train_loader.dataset)\n",
    "            #accuracy = 100. * correct / len(train_loader.dataset)\n",
    "            #print(accuracy1)\n",
    "            #print(len(accuracy1))\n",
    "            accuracy= sum(accuracy1) / len(accuracy1)\n",
    "         \n",
    "            #logx.msg('\\nShadowModel Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "             #   test_loss, correct, len(train_loader.dataset), accuracy))\n",
    "            logx.msg('\\nShadowModel Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, num_pred, accuracy))\n",
    "                # save model\n",
    "            if save:\n",
    "                save_dict = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': shadowmodel.state_dict(),\n",
    "                    'accuracy': accuracy}\n",
    "                logx.save_model(\n",
    "                    save_dict,\n",
    "                    metric=accuracy,\n",
    "                    epoch='',\n",
    "                higher_better=True)\n",
    "            print(action_list)\n",
    "            print(reward_list)\n",
    "            if action_list[j]==1:\n",
    "                state_list.append(1)\n",
    "                reward_list.append(5)\n",
    "            else:\n",
    "                state_list.append(0)\n",
    "                reward_list.append(-5)\n",
    "                \n",
    "            #print(state_list[0:20])\n",
    "            #print(state_list[1:21])\n",
    "            \n",
    "            dqn.store_transition(state_list[0:20],action_list[j],reward_list[j],state_list[1:21])\n",
    "            dqn.learn()\n",
    "            state_list.pop(0)\n",
    "            j=j+1\n",
    "\n",
    "def Train_Shadow_Model_ChangeDataSize(args):\n",
    "    dataset = 'CIFAR100'\n",
    "    split_size = [42000, 35000, 30000, 20000, 15000, 10000, 7000, 6000, 5000] \n",
    "    Nets = ['CNN3', 'CNN4', 'CNN5', 'CNN6', 'CNN7', 'CNN8', 'CNN9', 'CNN10', 'CNN11'] \n",
    "    targetmodel = CNN('CNN7', dataset)\n",
    "    targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "    state_dict, _ =  logx.load_model(path=args.logdir + '/target/7000/best_checkpoint_ep.pth')\n",
    "    targetmodel.load_state_dict(state_dict)\n",
    "    for net in Nets:\n",
    "        for _, cluster in enumerate(split_size):\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader = load_dataset(args, dataset, cluster, mode='ChangeDataSize')\n",
    "            shadowmodel = CNN(net, dataset)\n",
    "            shadowmodel.apply(init_func)\n",
    "            shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "            logx.initialize(logdir=args.logdir + '/ChangeDataSize/' + net + '/' + str(cluster), coolname=False, tensorboard=False)\n",
    "            optimizer = optim.Adam(shadowmodel.parameters(), lr=args.lr)\n",
    "            logx.msg('======================Train_Shadow_Model_ChangeDataSize Size: {}  Nets: {}===================='.format(cluster, net))\n",
    "            for epoch in range(1, args.epochs + 1):\n",
    "                train_shadow_model(args, targetmodel, shadowmodel, train_loader, optimizer, epoch)\n",
    "                test_shadow_model(args, targetmodel, shadowmodel, train_loader, epoch, save=True)\n",
    "\n",
    "# def AdversaryOne(args): ## loss or entropy or maximum\n",
    "#     logx.initialize(logdir=args.logdir + '/adversaryOne', coolname=False, tensorboard=False)\n",
    "#     split_size = args.Split_Size[args.dataset_ID]\n",
    "#     dataset = args.datasets[args.dataset_ID]\n",
    "#     AUC_Loss, AUC_Entropy, AUC_Maximum = [], [], []\n",
    "#     Distribution_Loss = []\n",
    "    \n",
    "#     for cluster in split_size:\n",
    "#         torch.cuda.empty_cache()\n",
    "#         args.batch_size = 1\n",
    "#         data_loader = load_dataset(args, dataset, cluster, mode='adversary', max_num=2000)\n",
    "\n",
    "#         targetmodel = CNN('CNN7', dataset)\n",
    "#         targetmodel.apply(init_func)\n",
    "#         targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "#         shadowmodel = CNN('CNN7', dataset)\n",
    "#         shadowmodel.apply(init_func)\n",
    "#         shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "\n",
    "#         state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "#         targetmodel.load_state_dict(state_dict)\n",
    "#         targetmodel.eval()\n",
    "#         state_dict, _ =  logx.load_model(path=args.logdir + '/shadow/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "#         shadowmodel.load_state_dict(state_dict)\n",
    "#         shadowmodel.eval()\n",
    "\n",
    "#         if args.advOne_metric == 'AUC':\n",
    "#             logx.msg('======================AdversaryOne AUC of Loss, Entropy, Maximum respectively cluster:{} ==================='.format(cluster))\n",
    "#             AUC_Loss, AUC_Entropy, AUC_Maximum = AdversaryOne_evaluation(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\n",
    "#         elif args.advOne_metric == 'Loss_visual':\n",
    "#             Distribution_Loss = AdversaryOne_Feature(args, shadowmodel, data_loader, cluster, Distribution_Loss)\n",
    "  \n",
    "#     df = pd.DataFrame()\n",
    "#     if args.advOne_metric == 'AUC':\n",
    "#         AUC_Loss = df.append(AUC_Loss, ignore_index=True)\n",
    "#         AUC_Entropy = df.append(AUC_Entropy, ignore_index=True)\n",
    "#         AUC_Maximum = df.append(AUC_Maximum, ignore_index=True)\n",
    "#         AUC_Loss.to_csv(args.logdir + '/adversaryOne/AUC_Loss.csv')\n",
    "#         AUC_Entropy.to_csv(args.logdir + '/adversaryOne/AUC_Entropy.csv')\n",
    "#         AUC_Maximum.to_csv(args.logdir + '/adversaryOne/AUC_Maximum.csv')\n",
    "#     else:\n",
    "#         Distribution_Loss = df.append(Distribution_Loss, ignore_index=True)\n",
    "#         Distribution_Loss.to_csv(args.logdir + '/adversaryOne/Distribution_Loss.csv')\n",
    "#     # Load the AUC data from the CSV files\n",
    "#     AUC_Loss = pd.read_csv(args.logdir + '/adversaryOne/AUC_Loss.csv')\n",
    "#     AUC_Entropy = pd.read_csv(args.logdir + '/adversaryOne/AUC_Entropy.csv')\n",
    "#     AUC_Maximum = pd.read_csv(args.logdir + '/adversaryOne/AUC_Maximum.csv')\n",
    "\n",
    "#     # Plot the AUC vs DataSize curves\n",
    "#     plt.plot(AUC_Loss['DataSize'], AUC_Loss['AUC'], label='AUC Loss')\n",
    "#     plt.plot(AUC_Entropy['DataSize'], AUC_Entropy['AUC'], label='AUC Entropy')\n",
    "#     plt.plot(AUC_Maximum['DataSize'], AUC_Maximum['AUC'], label='AUC Maximum')\n",
    "#     plt.xlabel('DataSize')\n",
    "#     plt.ylabel('AUC')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "def AdversaryOne(args): ## loss or entropy or maximum\n",
    "    logx.initialize(logdir=args.logdir + '/adversaryOne', coolname=False, tensorboard=False)\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    AUC_Loss, AUC_Entropy, AUC_Maximum = [], [], []\n",
    "    Distribution_Loss = []\n",
    "\n",
    "    for cluster in split_size:\n",
    "        torch.cuda.empty_cache()\n",
    "        args.batch_size = 1\n",
    "        data_loader = load_dataset(args, dataset, cluster, mode='adversary', max_num=2000)\n",
    "        \n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        targetmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        shadowmodel = CNN('CNN7', dataset)\n",
    "        shadowmodel.apply(init_func)\n",
    "        shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        targetmodel.eval()\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/shadow/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        shadowmodel.load_state_dict(state_dict)\n",
    "        shadowmodel.eval()\n",
    "\n",
    "        if args.advOne_metric == 'AUC':\n",
    "            logx.msg('======================AdversaryOne AUC of Loss, Entropy, Maximum respectively cluster:{} ==================='.format(cluster))\n",
    "            AUC_Loss, AUC_Entropy, AUC_Maximum = AdversaryOne_evaluation(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\n",
    "            #print('AUC_Loss: ', AUC_Loss)\n",
    "            #print('AUC_Entropy: ', AUC_Entropy)\n",
    "            #print('AUC_Maximum: ', AUC_Maximum)\n",
    "        elif args.advOne_metric == 'Loss_visual':\n",
    "            Distribution_Loss = AdversaryOne_Feature(args, shadowmodel, data_loader, cluster, Distribution_Loss)\n",
    "            #print('Distribution_Loss: ', Distribution_Loss)\n",
    "        print('finished')\n",
    "\n",
    "\n",
    "\n",
    "def AdversaryOne_ChangeDataSize(args):\n",
    "    split_size = [42000, 35000, 30000, 20000, 15000, 10000, 7000, 6000, 5000]\n",
    "    dataset = 'CIFAR100'\n",
    "    Nets = ['CNN3', 'CNN4', 'CNN5', 'CNN6', 'CNN7', 'CNN8', 'CNN9', 'CNN10', 'CNN11']\n",
    "    data_loader = load_dataset(args, dataset, 7000, mode='adversary', max_num=2000)\n",
    "    targetmodel = CNN('CNN7', dataset)\n",
    "    targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "    state_dict, _ =  logx.load_model(path=args.logdir + '/target/7000/best_checkpoint_ep.pth')\n",
    "    targetmodel.load_state_dict(state_dict)\n",
    "    targetmodel.eval()\n",
    "    for net in Nets:\n",
    "        AUC_Loss, AUC_Entropy, AUC_Maximum  = [], [], []\n",
    "        for _, cluster in enumerate(split_size):\n",
    "            torch.cuda.empty_cache()\n",
    "            shadowmodel = CNN(net, dataset)\n",
    "            shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "            state_dict, _ =  logx.load_model(path=args.logdir + '/ChangeDataSize/' + net + '/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "            shadowmodel.load_state_dict(state_dict)\n",
    "            shadowmodel.eval()\n",
    "            AUC_Loss, AUC_Entropy, AUC_Maximum = AdversaryOne_evaluation(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\n",
    "        df = pd.DataFrame()\n",
    "        AUC_Loss = df.append(AUC_Loss, ignore_index=True)\n",
    "        AUC_Loss.to_csv(args.logdir + '/ChangeDataSize/' + net + '/AUC_Loss.csv')\n",
    "\n",
    "\n",
    "\n",
    "def AdversaryTwo(args, Random_Data=False):\n",
    "    if Random_Data:\n",
    "        args.Split_Size = [[100], [2000], [100], [100]]\n",
    "        img_sizes = [(3,32,32), (3,32,32), (3,64,64), (3, 128, 128)] \n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    num_class = args.num_classes[args.dataset_ID]\n",
    "    \n",
    "    logx.initialize(logdir=args.logdir + '/adversaryTwo', coolname=False, tensorboard=False)\n",
    "    if args.blackadvattack == 'HopSkipJump':\n",
    "        ITER = [50] # for call HSJA evaluation [1, 5, 10, 15, 20, 30]  default 50\n",
    "    elif args.blackadvattack == 'QEBA':\n",
    "        ITER = [150] # for call QEBA evaluation default 150\n",
    "    elif args.blackadvattack == 'SaltandPepperNoise':\n",
    "        ITER = [-1] # for call SaltandPepperNoise evaluation default 150\n",
    "    for maxitr in ITER:\n",
    "        AUC_Dist, Distance = [], []\n",
    "        for cluster in split_size:\n",
    "            torch.cuda.empty_cache()\n",
    "            args.batch_size = 1\n",
    "            if Random_Data:\n",
    "                fake_set = datasets.FakeData(size=10000, image_size=img_sizes[args.dataset_ID], num_classes=num_class, transform= transforms.Compose([Rand_Augment(), transforms.ToTensor()]))\n",
    "                data_loader = DataLoader(fake_set, batch_size=args.batch_size, shuffle=False)\n",
    "            else:\n",
    "                data_loader = load_dataset(args, dataset, cluster, mode='adversary', max_num=200)\n",
    "            targetmodel = CNN('CNN7', dataset)\n",
    "            targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "            \n",
    "            state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "            targetmodel.load_state_dict(state_dict)\n",
    "            targetmodel.eval()\n",
    "            \n",
    "            if args.blackadvattack == 'HopSkipJump':\n",
    "                AUC_Dist, Distance = AdversaryTwo_HopSkipJump(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data, maxitr)\n",
    "            elif args.blackadvattack == 'QEBA':\n",
    "                AUC_Dist, Distance = AdversaryTwo_QEBA(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data, maxitr)\n",
    "            elif args.blackadvattack == 'SaltandPepperNoise':\n",
    "                AUC_Dist, Distance = AdversaryTwo_SaltandPepperNoise(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        AUC_Dist = df.append(AUC_Dist, ignore_index=True)\n",
    "        Distance = df.append(Distance, ignore_index=True)\n",
    "        \n",
    "        if Random_Data:\n",
    "            AUC_Dist.to_csv(args.logdir + '/adversaryTwo/AUC_Dist_'+args.blackadvattack+'.csv')\n",
    "            Distance.to_csv(args.logdir + '/adversaryTwo/Distance_Random_'+args.blackadvattack+'.csv')\n",
    "        else:\n",
    "            AUC_Dist.to_csv(args.logdir + '/adversaryTwo/AUC_Dist_'+args.blackadvattack + '.csv')\n",
    "            Distance.to_csv(args.logdir + '/adversaryTwo/Distance_'+args.blackadvattack+'.csv')\n",
    "        \n",
    "\n",
    "def Decision_Radius(args):\n",
    "    num_class = args.num_classes[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "\n",
    "    for _, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        mem_set, non_set, transform = load_dataset(args, dataset, cluster, mode='radius')\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        targetmodel.eval()\n",
    "\n",
    "        logx.initialize(logdir=args.logdir + '/radius/' + str(cluster), coolname=False, tensorboard=False)\n",
    "\n",
    "        max_num = 200 if 200 < len(mem_set) else len(mem_set)\n",
    "        logx.msg('======================Starting Decision Radius Training Dataset ====================')\n",
    "        certify(targetmodel, 'cuda', mem_set, transform, num_class,\n",
    "                    mode='both', start_img=0, num_img=max_num, \n",
    "                    sigma=0.25, beta=16)\n",
    "\n",
    "        logx.msg('======================Starting Decision Radius Testing Dataset ====================')\n",
    "        certify(targetmodel, 'cuda', non_set, transform, num_class,\n",
    "                mode='both', start_img=0, num_img=max_num, \n",
    "                sigma=0.25, beta=16)\n",
    "\n",
    "\n",
    "##############################\n",
    "def main(): \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Decision-based Membership Inference Attack Toy Example') \n",
    "    parser.add_argument('--train', default=True, type=bool,\n",
    "                        help='train or attack')\n",
    "    parser.add_argument('--dataset_ID', default=False, type=int, \n",
    "                        help='CIFAR10=0, CIFAR100=1, GTSRB=2, Face=3')\n",
    "    parser.add_argument('--datasets', nargs='+',\n",
    "                        default=['CIFAR10', 'CIFAR100', 'GTSRB', 'Face'])\n",
    "    parser.add_argument('--num_classes', nargs='+',\n",
    "                        default=[10, 100, 43, 19])\n",
    "    parser.add_argument('--Split-Size', nargs='+',\n",
    "                        default=[[3000, 2000, 1500, 1000, 500, 100],                     #3000, 2000, 1500, 1000, 500, 100\n",
    "                                [7000, 6000, 5000, 4000, 3000, 2000 ],                      #9000, 8000, 7000, 6000, 5000, 4000  # 7000, 6000, 5000, 4000, 3000, 2000\n",
    "                                [600, 500, 400, 300, 200, 100  ],  #600, 500, 400, 300, 200, 100            \n",
    "                                [350, 300, 250, 200, 150, 100  ],  #350, 300, 250, 200, 150, 100                \n",
    "                                ]) \n",
    "    parser.add_argument('--batch-size', nargs='+', default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=200, metavar='N',\n",
    "                        help='number of epochs to train (default: 200)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001 for adam; 0.1 for SGD)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--cuda', default=True,type=bool,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--blackadvattack', default='HopSkipJump', type=str,\n",
    "                        help='adversaryTwo uses the adv attack the target Model: HopSkipJump; QEBA')\n",
    "    parser.add_argument('--logdir', type=str, default='',\n",
    "                        help='target log directory')\n",
    "    parser.add_argument('--mode_type', type=str, default='',\n",
    "                        help='the type of action referring to the load dataset')\n",
    "    parser.add_argument('--advOne_metric', type=str, default='AUC', help='AUC of Loss, Entropy, Maximum respectively; or Loss_visual')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    for dataset_idx in [0,1]:\n",
    "        args.dataset_ID = dataset_idx\n",
    "        args.logdir = 'results'+'/' + args.datasets[args.dataset_ID]\n",
    "        action = 3\n",
    "        # train\n",
    "        if action == 0:\n",
    "            args.mode_type = 'target'\n",
    "            Train_Target_Model(args)\n",
    "        elif action == 1:\n",
    "            args.mode_type = 'shadow'\n",
    "            Train_Shadow_Model(args)\n",
    "        elif action == 2: \n",
    "            args.logdir = 'results/CIFAR100' \n",
    "            Train_Shadow_Model_ChangeDataSize(args)\n",
    "        # attack\n",
    "        elif action == 3:\n",
    "            AdversaryOne(args)\n",
    "        elif action == 4:    \n",
    "            args.logdir = 'results/CIFAR100'  \n",
    "            AdversaryOne_ChangeDataSize(args)\n",
    "        elif action == 5:\n",
    "            AdversaryTwo(args, Random_Data=False)\n",
    "\n",
    "        # others\n",
    "        elif action == 6:\n",
    "            Decision_Radius(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ef9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b99433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
