{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7f21a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'runx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrunx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logx\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'runx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from runx.logx import logx\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from models import ResNet18\n",
    "from classifier import CNN\n",
    "from utils import load_dataset, init_func, Rand_Augment\n",
    "from deeplearning import train_target_model, test_target_model, train_shadow_model, test_shadow_model\n",
    "from attack import AdversaryOne_Feature, AdversaryOne_evaluation, AdversaryTwo_HopSkipJump, AdversaryTwo_QEBA, AdversaryTwo_SaltandPepperNoise\n",
    "from cert_radius.certify import certify\n",
    "\n",
    "\n",
    "action = -1\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Net class (Define Network)\n",
    "\n",
    "N_ACTIONS = 2               \n",
    "N_STATES = 20\n",
    "\n",
    "BATCH_SIZE = 32                                 # Number of samples\n",
    "LR = 0.01                                       # Learning rate\n",
    "EPSILON = 0.99                                   # Greedy policy\n",
    "GAMMA = 0.9                                     # Reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # Target network update frequency\n",
    "MEMORY_CAPACITY = 2000                          # Memory capacity\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                                                         # Define a series of attributes for Net\n",
    "        super(Net, self).__init__()                                             # Equivalent to nn.Module.__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)                                      # Set the first fully connected layer (input layer to hidden layer): from state neurons to 50 neurons\n",
    "        self.fc1.weight.data.normal_(0, 0.1)                                    # Weight initialization (normal distribution with mean 0, variance 0.1)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)                                     # Set the second fully connected layer (hidden layer to output layer): from 50 neurons to action neurons\n",
    "        self.out.weight.data.normal_(0, 0.1)                                    # Weight initialization (normal distribution with mean 0, variance 0.1)\n",
    "\n",
    "    def forward(self, x):                                                       # Define forward function (x is state)\n",
    "        x = F.relu(self.fc1(x))                                                 # Connect input layer to hidden layer, and use ReLU activation function to process values from hidden layer\n",
    "        actions_value = self.out(x)                                             # Connect hidden layer to output layer, get the final output value (i.e., action value)\n",
    "        return actions_value \n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self):                                                         # Define a series of attributes for DQN\n",
    "        self.eval_net, self.target_net = Net(), Net()                           # Use Net to create two neural networks: evaluation network and target network\n",
    "        self.learn_step_counter = 0                                             # For target updating\n",
    "        self.memory_counter = 0                                                 # For storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))             # Initialize memory, one row represents one transition\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # Use Adam optimizer (input is the parameters of evaluation network and learning rate)\n",
    "        self.loss_func = nn.MSELoss()                                           # Use Mean Squared Error loss function (loss(xi, yi)=(xi-yi)^2)\n",
    "\n",
    "    def choose_action(self, x):                                                 # Define action selection function (x is state)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # Convert x to 32-bit floating point form, and add a dimension of size 1 at dim=0\n",
    "        if np.random.uniform() < EPSILON:                                       # Generate a random number in [0, 1), if less than EPSILON, choose the optimal action\n",
    "            actions_value = self.eval_net.forward(x)                            # Get action values by forward propagation of evaluation network with input state x\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                # Output the index of the maximum value of each row, and convert to numpy ndarray form\n",
    "            action = action[0]                                                  # Output the first number of action\n",
    "        else:                                                                   # Randomly choose action\n",
    "            action = np.random.randint(0, N_ACTIONS)                            # Here action is randomly equal to 0 or 1 (N_ACTIONS = 2)\n",
    "        return action                                                           # Return the selected action (0 or 1)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):                                    # Define memory storage function (here input is a transition)\n",
    "        transition = np.hstack((s, [a, r], s_))                                 # Concatenate arrays horizontally\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           # Get the row number where transition will be placed\n",
    "        self.memory[index, :] = transition                                      # Place the transition\n",
    "        self.memory_counter += 1                                                # Increment memory_counter by 1\n",
    "\n",
    "    def learn(self):                                                            # Define learning function (start learning after memory is full)\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # Trigger at the beginning, then trigger every 100 steps\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         # Assign parameters of evaluation network to target network\n",
    "        self.learn_step_counter += 1                                            # Increment learn_step_counter by 1\n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            # Randomly select 32 numbers in [0, 2000), may repeat\n",
    "        b_memory = self.memory[sample_index, :]                                 # Extract 32 transitions corresponding to 32 indexes, store in b_memory\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()                                      # Clear the residual updated parameters from the previous step\n",
    "        loss.backward()                                                 # Backpropagation of error, calculate parameter update value\n",
    "        self.optimizer.step()\n",
    "        \n",
    "def Train_Target_Model(args):\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    dqn = DQN()\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    for idx, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache() \n",
    "        logx.initialize(logdir=args.logdir + '/target/' + str(cluster), coolname=False, tensorboard=False)\n",
    "        train_loader, test_loader = load_dataset(args, dataset, cluster, mode=args.mode_type)\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        targetmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        optimizer = optim.Adam(targetmodel.parameters(), lr=args.lr)\n",
    "        logx.msg('======================Train_Target_Model {} ===================='.format(cluster))\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train_target_model(args, targetmodel, train_loader, optimizer, epoch)\n",
    "            test_target_model(args, targetmodel, test_loader, epoch, save=True)\n",
    "\n",
    "\n",
    "def Train_Shadow_Model(args):\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    save = True\n",
    "    for idx, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        train_loader = load_dataset(args, dataset, cluster, mode=args.mode_type)\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        shadowmodel = CNN('CNN7', dataset)\n",
    "        \n",
    "        targetmodel.apply(init_func)\n",
    "        shadowmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "        \n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        dqn = DQN()\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list = []\n",
    "        \n",
    "        for i in range(20):\n",
    "            state_list.append(1)\n",
    "            \n",
    "        logx.initialize(logdir=args.logdir + '/shadow/'+ str(cluster), coolname=False, tensorboard=False)\n",
    "        optimizer = optim.Adam(shadowmodel.parameters(), lr=args.lr)\n",
    "        logx.msg('======================Train_Shadow_Model {} ===================='.format(cluster))\n",
    "        \n",
    "        j = 0\n",
    "        accuracy1 = []\n",
    "        num_pred = 0\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            \n",
    "            if j <= 20:\n",
    "                action_list.append(1)\n",
    "            else:\n",
    "                action_list.append(dqn.choose_action(state_list[j]))\n",
    "\n",
    "            targetmodel.eval()\n",
    "            shadowmodel.train()\n",
    "                               \n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = data.cuda()\n",
    "                output = targetmodel(data)\n",
    "                _, target = output.max(1)\n",
    "                optimizer.zero_grad()\n",
    "                output = shadowmodel(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch_idx % args.log_interval == 0:\n",
    "                    logx.msg('ShadowModel Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "            \n",
    "            targetmodel.eval()\n",
    "            shadowmodel.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                    data = data.cuda()\n",
    "                    output = targetmodel(data)\n",
    "                    _, target = output.max(1)\n",
    "\n",
    "                    output = shadowmodel(data)\n",
    "                    test_loss += F.cross_entropy(output, target).item()\n",
    "                    pred = output.max(1, keepdim=True)[1]\n",
    "                     \n",
    "                    reference_tensor = pred\n",
    "                    \n",
    "                    if action_list[j] == 1:\n",
    "                        random_array = np.random.randint(0, 10, size=(128, 1))\n",
    "                        pred = torch.from_numpy(random_array).cuda()\n",
    "                        pred = pred[:reference_tensor.shape[0]]\n",
    "                        for k in range(len(target)):\n",
    "                            target = torch.cat([target[:k], target[k+1:]], dim=0)\n",
    "                            pred = torch.cat([pred[:k], pred[k+1:]], dim=0)\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                    accuracy1.append(correct / len(pred))\n",
    "                    num_pred += len(pred)\n",
    "            test_loss /= len(train_loader.dataset)\n",
    "            accuracy = sum(accuracy1) / len(accuracy1)\n",
    "         \n",
    "            logx.msg('\\nShadowModel Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, num_pred, accuracy))\n",
    "\n",
    "            if save:\n",
    "                save_dict = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': shadowmodel.state_dict(),\n",
    "                    'accuracy': accuracy}\n",
    "                logx.save_model(\n",
    "                    save_dict,\n",
    "                    metric=accuracy,\n",
    "                    epoch='',\n",
    "                    higher_better=True)\n",
    "            \n",
    "            if action_list[j] == 1:\n",
    "                state_list.append(1)\n",
    "                reward_list.append(5)\n",
    "            else:\n",
    "                state_list.append(0)\n",
    "                reward_list.append(-5)\n",
    "            \n",
    "            dqn.store_transition(state_list[0:20], action_list[j], reward_list[j], state_list[1:21])\n",
    "            dqn.learn()\n",
    "            state_list.pop(0)\n",
    "            j += 1\n",
    "\n",
    "def Train_Shadow_Model_ChangeDataSize(args):\n",
    "    dataset = 'CIFAR100'\n",
    "    split_size = [42000, 35000, 30000, 20000, 15000, 10000, 7000, 6000, 5000] \n",
    "    Nets = ['CNN3', 'CNN4', 'CNN5', 'CNN6', 'CNN7', 'CNN8', 'CNN9', 'CNN10', 'CNN11'] \n",
    "    targetmodel = CNN('CNN7', dataset)\n",
    "    targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "    state_dict, _ =  logx.load_model(path=args.logdir + '/target/7000/best_checkpoint_ep.pth')\n",
    "    targetmodel.load_state_dict(state_dict)\n",
    "    for net in Nets:\n",
    "        for _, cluster in enumerate(split_size):\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loader = load_dataset(args, dataset, cluster, mode='ChangeDataSize')\n",
    "            shadowmodel = CNN(net, dataset)\n",
    "            shadowmodel.apply(init_func)\n",
    "            shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "            logx.initialize(logdir=args.logdir + '/ChangeDataSize/' + net + '/' + str(cluster), coolname=False, tensorboard=False)\n",
    "            optimizer = optim.Adam(shadowmodel.parameters(), lr=args.lr)\n",
    "            logx.msg('======================Train_Shadow_Model_ChangeDataSize Size: {}  Nets: {}===================='.format(cluster, net))\n",
    "            for epoch in range(1, args.epochs + 1):\n",
    "                train_shadow_model(args, targetmodel, shadowmodel, train_loader, optimizer, epoch)\n",
    "                test_shadow_model(args, targetmodel, shadowmodel, train_loader, epoch, save=True)\n",
    "\n",
    "def AdversaryOne(args): ## loss or entropy or maximum\n",
    "    logx.initialize(logdir=args.logdir + '/adversaryOne', coolname=False, tensorboard=False)\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    AUC_Loss, AUC_Entropy, AUC_Maximum = [], [], []\n",
    "    Distribution_Loss = []\n",
    "\n",
    "    for cluster in split_size:\n",
    "        torch.cuda.empty_cache()\n",
    "        args.batch_size = 1\n",
    "        data_loader = load_dataset(args, dataset, cluster, mode='adversary', max_num=2000)\n",
    "        \n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "        targetmodel.apply(init_func)\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        shadowmodel = CNN('CNN7', dataset)\n",
    "        shadowmodel.apply(init_func)\n",
    "        shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        targetmodel.eval()\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/shadow/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        shadowmodel.load_state_dict(state_dict)\n",
    "        shadowmodel.eval()\n",
    "\n",
    "        if args.advOne_metric == 'AUC':\n",
    "            logx.msg('======================AdversaryOne AUC of Loss, Entropy, Maximum respectively cluster:{} ==================='.format(cluster))\n",
    "            AUC_Loss, AUC_Entropy, AUC_Maximum = AdversaryOne_evaluation(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\n",
    "        elif args.advOne_metric == 'Loss_visual':\n",
    "            Distribution_Loss = AdversaryOne_Feature(args, shadowmodel, data_loader, cluster, Distribution_Loss)\n",
    "        print('finished')\n",
    "\n",
    "\n",
    "def AdversaryOne_ChangeDataSize(args):\n",
    "    split_size = [42000, 35000, 30000, 20000, 15000, 10000, 7000, 6000, 5000]\n",
    "    dataset = 'CIFAR100'\n",
    "    Nets = ['CNN3', 'CNN4', 'CNN5', 'CNN6', 'CNN7', 'CNN8', 'CNN9', 'CNN10', 'CNN11']\n",
    "    data_loader = load_dataset(args, dataset, 7000, mode='adversary', max_num=2000)\n",
    "    targetmodel = CNN('CNN7', dataset)\n",
    "    targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "    state_dict, _ =  logx.load_model(path=args.logdir + '/target/7000/best_checkpoint_ep.pth')\n",
    "    targetmodel.load_state_dict(state_dict)\n",
    "    targetmodel.eval()\n",
    "    for net in Nets:\n",
    "        AUC_Loss, AUC_Entropy, AUC_Maximum  = [], [], []\n",
    "        for _, cluster in enumerate(split_size):\n",
    "            torch.cuda.empty_cache()\n",
    "            shadowmodel = CNN(net, dataset)\n",
    "            shadowmodel = nn.DataParallel(shadowmodel.cuda())\n",
    "            state_dict, _ =  logx.load_model(path=args.logdir + '/ChangeDataSize/' + net + '/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "            shadowmodel.load_state_dict(state_dict)\n",
    "            shadowmodel.eval()\n",
    "            AUC_Loss, AUC_Entropy, AUC_Maximum = AdversaryOne_evaluation(args, targetmodel, shadowmodel, data_loader, cluster, AUC_Loss, AUC_Entropy, AUC_Maximum)\n",
    "        df = pd.DataFrame()\n",
    "        AUC_Loss = df.append(AUC_Loss, ignore_index=True)\n",
    "        AUC_Loss.to_csv(args.logdir + '/ChangeDataSize/' + net + '/AUC_Loss.csv')\n",
    "\n",
    "\n",
    "\n",
    "def AdversaryTwo(args, Random_Data=False):\n",
    "    if Random_Data:\n",
    "        args.Split_Size = [[100], [2000], [100], [100]]\n",
    "        img_sizes = [(3,32,32), (3,32,32), (3,64,64), (3, 128, 128)] \n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    num_class = args.num_classes[args.dataset_ID]\n",
    "    \n",
    "    logx.initialize(logdir=args.logdir + '/adversaryTwo', coolname=False, tensorboard=False)\n",
    "    if args.blackadvattack == 'HopSkipJump':\n",
    "        ITER = [50]\n",
    "    elif args.blackadvattack == 'QEBA':\n",
    "        ITER = [150]\n",
    "    elif args.blackadvattack == 'SaltandPepperNoise':\n",
    "        ITER = [-1]\n",
    "    for maxitr in ITER:\n",
    "        AUC_Dist, Distance = [], []\n",
    "        for cluster in split_size:\n",
    "            torch.cuda.empty_cache()\n",
    "            args.batch_size = 1\n",
    "            if Random_Data:\n",
    "                fake_set = datasets.FakeData(size=10000, image_size=img_sizes[args.dataset_ID], num_classes=num_class, transform= transforms.Compose([Rand_Augment(), transforms.ToTensor()]))\n",
    "                data_loader = DataLoader(fake_set, batch_size=args.batch_size, shuffle=False)\n",
    "            else:\n",
    "                data_loader = load_dataset(args, dataset, cluster, mode='adversary', max_num=200)\n",
    "            targetmodel = CNN('CNN7', dataset)\n",
    "            targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "            \n",
    "            state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "            targetmodel.load_state_dict(state_dict)\n",
    "            targetmodel.eval()\n",
    "            \n",
    "            if args.blackadvattack == 'HopSkipJump':\n",
    "                AUC_Dist, Distance = AdversaryTwo_HopSkipJump(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data, maxitr)\n",
    "            elif args.blackadvattack == 'QEBA':\n",
    "                AUC_Dist, Distance = AdversaryTwo_QEBA(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data, maxitr)\n",
    "            elif args.blackadvattack == 'SaltandPepperNoise':\n",
    "                AUC_Dist, Distance = AdversaryTwo_SaltandPepperNoise(args, targetmodel, data_loader, cluster, AUC_Dist, Distance, Random_Data)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        AUC_Dist = df.append(AUC_Dist, ignore_index=True)\n",
    "        Distance = df.append(Distance, ignore_index=True)\n",
    "        \n",
    "        if Random_Data:\n",
    "            AUC_Dist.to_csv(args.logdir + '/adversaryTwo/AUC_Dist_'+args.blackadvattack+'.csv')\n",
    "            Distance.to_csv(args.logdir + '/adversaryTwo/Distance_Random_'+args.blackadvattack+'.csv')\n",
    "        else:\n",
    "            AUC_Dist.to_csv(args.logdir + '/adversaryTwo/AUC_Dist_'+args.blackadvattack + '.csv')\n",
    "            Distance.to_csv(args.logdir + '/adversaryTwo/Distance_'+args.blackadvattack+'.csv')\n",
    "        \n",
    "\n",
    "def Decision_Radius(args):\n",
    "    num_class = args.num_classes[args.dataset_ID]\n",
    "    dataset = args.datasets[args.dataset_ID]\n",
    "    split_size = args.Split_Size[args.dataset_ID]\n",
    "    \n",
    "    for _, cluster in enumerate(split_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        mem_set, non_set, transform = load_dataset(args, dataset, cluster, mode='radius')\n",
    "        targetmodel = CNN('CNN7', dataset)\n",
    "\n",
    "        targetmodel = nn.DataParallel(targetmodel.cuda())\n",
    "        state_dict, _ =  logx.load_model(path=args.logdir + '/target/' + str(cluster) + '/best_checkpoint_ep.pth')\n",
    "        targetmodel.load_state_dict(state_dict)\n",
    "        targetmodel.eval()\n",
    "\n",
    "        logx.initialize(logdir=args.logdir + '/radius/' + str(cluster), coolname=False, tensorboard=False)\n",
    "\n",
    "        max_num = 200 if 200 < len(mem_set) else len(mem_set)\n",
    "        logx.msg('======================Starting Decision Radius Training Dataset ====================')\n",
    "        certify(targetmodel, 'cuda', mem_set, transform, num_class,\n",
    "                    mode='both', start_img=0, num_img=max_num, \n",
    "                    sigma=0.25, beta=16)\n",
    "\n",
    "        logx.msg('======================Starting Decision Radius Testing Dataset ====================')\n",
    "        certify(targetmodel, 'cuda', non_set, transform, num_class,\n",
    "                mode='both', start_img=0, num_img=max_num, \n",
    "                sigma=0.25, beta=16)\n",
    "\n",
    "\n",
    "##############################\n",
    "def main(): \n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Decision-based Membership Inference Attack Toy Example') \n",
    "    parser.add_argument('--train', default=True, type=bool,\n",
    "                        help='train or attack')\n",
    "    parser.add_argument('--dataset_ID', default=2, type=int, \n",
    "                        help='CIFAR10=0, CIFAR100=1, GTSRB=2, Face=3')\n",
    "    parser.add_argument('--datasets', nargs='+',\n",
    "                        default=['CIFAR10', 'CIFAR100', 'GTSRB', 'Face'])\n",
    "    parser.add_argument('--num_classes', nargs='+',\n",
    "                        default=[10, 100, 43, 19])\n",
    "    parser.add_argument('--Split-Size', nargs='+',\n",
    "                        default=[[3000, 2000, 1500, 1000, 500, 100],                     \n",
    "                                [7000, 6000, 5000, 4000, 3000, 2000],  \n",
    "                                [600, 500, 400, 300, 200, 100],             \n",
    "                                [350, 300, 250, 200, 150, 100]]) \n",
    "    parser.add_argument('--batch-size', nargs='+', default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=200, metavar='N',\n",
    "                        help='number of epochs to train (default: 200)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.001 for adam; 0.1 for SGD)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--cuda', default=True, type=bool,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--blackadvattack', default='HopSkipJump', type=str,\n",
    "                        help='adversaryTwo uses the adv attack the target Model: HopSkipJump; QEBA')\n",
    "    parser.add_argument('--logdir', type=str, default='',\n",
    "                        help='target log directory')\n",
    "    parser.add_argument('--mode_type', type=str, default='',\n",
    "                        help='the type of action referring to the load dataset')\n",
    "    parser.add_argument('--advOne_metric', type=str, default='AUC', help='AUC of Loss, Entropy, Maximum respectively; or Loss_visual')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    for dataset_idx in [0,1]:\n",
    "        args.dataset_ID = dataset_idx\n",
    "        args.logdir = 'results' + '/' + args.datasets[args.dataset_ID]\n",
    "        action = 3\n",
    "        if action == 0:\n",
    "            args.mode_type = 'target'\n",
    "            Train_Target_Model(args)\n",
    "        elif action == 1:\n",
    "            args.mode_type = 'shadow'\n",
    "            Train_Shadow_Model(args)\n",
    "        elif action == 2: \n",
    "            args.logdir = 'results/CIFAR100' \n",
    "            Train_Shadow_Model_ChangeDataSize(args)\n",
    "        elif action == 3:\n",
    "            AdversaryOne(args)\n",
    "        elif action == 4:    \n",
    "            args.logdir = 'results/CIFAR100'  \n",
    "            AdversaryOne_ChangeDataSize(args)\n",
    "        elif action == 5:\n",
    "            AdversaryTwo(args, Random_Data=False)\n",
    "        elif action == 6:\n",
    "            Decision_Radius(args)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ef9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b99433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
