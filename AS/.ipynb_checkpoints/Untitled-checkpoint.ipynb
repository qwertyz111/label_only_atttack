{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09392780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils_rb import *\n",
    "from ramboattack import RamBoAtt\n",
    "from HSJA_rb import HSJA\n",
    "from SignOPT_rb import OPT_attack_sign_SGD\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "action = -1\n",
    "\n",
    "# Define the Net class (define the network)\n",
    "\n",
    "N_ACTIONS = 10\n",
    "N_STATES = 6144  # Adjusted for concatenated image input\n",
    "\n",
    "BATCH_SIZE = 32                                  # Number of samples\n",
    "LR = 0.01                                        # Learning rate\n",
    "EPSILON = 0.99                                   # Greedy policy\n",
    "GAMMA = 0.9                                      # Reward discount\n",
    "TARGET_REPLACE_ITER = 100                        # Frequency of target network update\n",
    "MEMORY_CAPACITY = 2000                           # Memory capacity\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                          # Define a series of attributes for Net\n",
    "        super(Net, self).__init__()              # Equivalent to nn.Module.__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(N_STATES, 50)       # Set the first fully connected layer (input layer to hidden layer): from N_STATES neurons to 50 neurons\n",
    "        self.fc1.weight.data.normal_(0, 0.1)     # Weight initialization (normal distribution with mean 0 and standard deviation 0.1)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)      # Set the second fully connected layer (hidden layer to output layer): from 50 neurons to N_ACTIONS neurons\n",
    "        self.out.weight.data.normal_(0, 0.1)     # Weight initialization (normal distribution with mean 0 and standard deviation 0.1)\n",
    "\n",
    "    def forward(self, x):                        # Define the forward function (x is the state)\n",
    "        x = F.relu(self.fc1(x))                  # Connect the input layer to the hidden layer, and use the ReLU activation function to process the value after the hidden layer\n",
    "        actions_value = self.out(x)              # Connect the hidden layer to the output layer, and get the final output value (i.e., action value)\n",
    "        return actions_value\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):                          # Define a series of attributes for DQN\n",
    "        self.eval_net, self.target_net = Net(), Net()  # Create two neural networks using Net: evaluation network and target network\n",
    "        self.learn_step_counter = 0              # For target updating\n",
    "        self.memory_counter = 0                  # For storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  # Initialize the memory, each row represents a transition\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)  # Use the Adam optimizer (input is the evaluation network parameters and learning rate)\n",
    "        self.loss_func = nn.MSELoss()            # Use mean squared error loss function (loss(xi, yi) = (xi - yi)^2)\n",
    "\n",
    "    def choose_action(self, x):                  # Define the action selection function (x is the state)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)  # Convert x to 32-bit floating point format and add a dimension of 1 at dim=0\n",
    "        if np.random.uniform() < EPSILON:        # Generate a random number in [0, 1), if less than EPSILON, choose the optimal action\n",
    "            actions_value = self.eval_net.forward(x)  # Get the action value by feeding the state x to the evaluation network\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()  # Output the index of the maximum value in each row, and convert to numpy ndarray format\n",
    "            action = action[0]                   # Output the first number of action\n",
    "        else:                                    # Randomly choose an action\n",
    "            action = np.random.randint(0, N_ACTIONS)  # Here action is randomly 0 or 1 (N_ACTIONS = 2)\n",
    "        return action                            # Return the chosen action (0 or 1)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):     # Define the memory storage function (here input is a transition)\n",
    "        transition = np.hstack((s, [a, r], s_))  # Concatenate arrays horizontally\n",
    "        # If the memory is full, overwrite old data\n",
    "        index = self.memory_counter % MEMORY_CAPACITY  # Get the row number where the transition will be placed\n",
    "        self.memory[index, :] = transition       # Place the transition\n",
    "        self.memory_counter += 1                 # Increment memory_counter by 1\n",
    "\n",
    "    def learn(self):                             # Define the learning function (start learning after the memory is full)\n",
    "        # Target network parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:  # Trigger at the beginning, then every 100 steps\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())  # Assign the evaluation network parameters to the target network\n",
    "        self.learn_step_counter += 1             # Increment learn_step_counter by 1\n",
    "\n",
    "        # Sample a batch of data from the memory\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)  # Randomly sample 32 numbers from [0, 2000), may repeat\n",
    "        b_memory = self.memory[sample_index, :]   # Extract 32 transitions corresponding to the 32 indices, store in b_memory\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        # Extract 32 states, convert to 32-bit floating point format, and store in b_s, b_s has 32 rows and 4 columns\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        # Extract 32 actions, convert to 64-bit integer (signed) format, and store in b_a (it is LongTensor type to facilitate the use of torch.gather), b_a has 32 rows and 1 column\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        # Extract 32 rewards, convert to 32-bit floating point format, and store in b_r, b_r has 32 rows and 1 column\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        # Extract 32 next states, convert to 32-bit floating point format, and store in b_s_, b_s_ has 32 rows and 4 columns\n",
    "\n",
    "        # Get the evaluation and target values for 32 transitions, and update the evaluation network parameters using the loss function and optimizer\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "        # eval_net(b_s) outputs a series of action values for each b_s through the evaluation network, then .gather(1, b_a) aggregates the Q values of the corresponding indices b_a for each row\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        # q_next does not propagate the error backward, so detach; q_next represents a series of action values for each b_s_ output by the target network\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        # q_next.max(1)[0] means only returning the maximum value of each row, not the index (a one-dimensional tensor of length 32); .view() converts the previous one-dimensional tensor into the shape of (BATCH_SIZE, 1); finally get the target value through the formula\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        # Input 32 evaluation values and 32 target values, use mean squared error loss function\n",
    "        self.optimizer.zero_grad()               # Clear the residual update parameter values of the previous step\n",
    "        loss.backward()                          # Backpropagate the error, calculate the parameter update value\n",
    "        self.optimizer.step()                    # Execute a single optimization step (parameter update)\n",
    "        \n",
    "def imshow(img):\n",
    "    npimg = img[0].cpu().numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "    \n",
    "def perturbation_heat_map(xo, xa):\n",
    "    fig_dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    x = torch.abs(xo - xa).sum(dim=1).cpu()[0]\n",
    "    sns.heatmap(x, ax=ax, xticklabels=False, yticklabels=False, cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "# a. Load dataset\n",
    "\n",
    "batch_size = 32\n",
    "dataset = 'cifar10' \n",
    "datapath = '../datasets/cifar10'\n",
    "testloader, testset = load_data(dataset, data_path=datapath, batch_size=batch_size)\n",
    "\n",
    "# b. Load pre-trained model\n",
    "\n",
    "# 'resnet50' if pre-trained model from Pytorch. 'cifar10' if using pre-trained cifar10 model\n",
    "arch = 'cifar10' \n",
    "\n",
    "# None means using pre-trained model from Pytorch or default path. Otherwise, please change model_path = '...'\n",
    "model_path = None \n",
    "\n",
    "# True means pre-trained model does \"not\" normalized data while training, \n",
    "# so no need to unnorm during inference (used for CIFAR10 model)\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    num_classes = 10\n",
    "    unnorm = True # True means pre-trained model does \"not\" normalized data while training.\n",
    "    \n",
    "net = load_model(arch, model_path)\n",
    "model_rb = PretrainedModel(net, dataset, unnorm)\n",
    "\n",
    "bounds = [0, 1]\n",
    "model_ex = PytorchModel_ex(net, bounds, num_classes, dataset, unnorm)\n",
    "\n",
    "# c. Load evaluation set\n",
    "targeted = True # True means targeted attack. False means untargeted attack.\n",
    "# 'balance', 'easyset'->imagenet or cifar10; \n",
    "# 'hardset'-> imagenet; \n",
    "# 'hardset_A','hardset_B','hardset_D' -> cifar10\n",
    "eval_set =  'hardset_A'\n",
    "\n",
    "ID_set = get_evalset(dataset, targeted, eval_set)\n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "i = 2 #0,1,2,10,20, 50,123 # the sample i-th in the evaluation set\n",
    "query_limit = 50000\n",
    "D = np.zeros(query_limit + 2000)\n",
    "nquery = 0\n",
    "o = ID_set[i, 1] #oID\n",
    "\n",
    "# 0. select original image\n",
    "oimg, olabel = testset[o]\n",
    "oimg = torch.unsqueeze(oimg, 0).cuda()\n",
    "\n",
    "# 1. select starting image\n",
    "if targeted:\n",
    "    t = ID_set[i, 3] #tID, 3 is index across dataset - 4 is sample index in a class (not across dataset)\n",
    "    tlabel = ID_set[i, 2]\n",
    "    timg, _ = testset[t]\n",
    "    timg = torch.unsqueeze(timg, 0).cuda()\n",
    "    y_targ = np.array([tlabel])\n",
    "else:\n",
    "    tlabel = None\n",
    "    y_targ = np.array([olabel])\n",
    "    \n",
    "# ============= 2. setup ==============\n",
    "if dataset == 'cifar10':\n",
    "    delta = 1e-2\n",
    "    len_T = 500\n",
    "\n",
    "constraint = 'l2'\n",
    "num_iterations = 150\n",
    "gamma = 1.0\n",
    "stepsize_search = 'geometric_progression'\n",
    "max_num_evals = 1e4\n",
    "init_num_evals = 100\n",
    "verbose = True\n",
    "auto_terminate = False\n",
    "\n",
    "model_ex = PytorchModel_ex(net, bounds, num_classes, dataset, unnorm)\n",
    "module = HSJA(model_ex, constraint, num_iterations, gamma, stepsize_search, max_num_evals, init_num_evals, verbose, delta, len_T)\n",
    "\n",
    "# Use DQN to change the label during the attack process\n",
    "state = np.concatenate((oimg.cpu().numpy().flatten(), timg.cpu().numpy().flatten()))\n",
    "action = dqn.choose_action(state)\n",
    "new_label = action  # DQN selects the new label\n",
    "\n",
    "if targeted:\n",
    "    adv, nqry, Dt = module.hsja(oimg.cpu().numpy(), np.array([new_label]), timg.cpu().numpy(), targeted, query_limit, auto_terminate)\n",
    "else:\n",
    "    timg = None\n",
    "    adv, nqry, Dt = module.hsja(oimg.cpu().numpy(), np.array([new_label]), timg, targeted, auto_terminate)\n",
    "    \n",
    "adv = torch.unsqueeze(torch.from_numpy(adv).float(), 0).cuda()\n",
    "\n",
    "print('Source image:')\n",
    "imshow(oimg)\n",
    "\n",
    "print('Starting image:')\n",
    "timg, _ = testset[t]\n",
    "timg = torch.unsqueeze(timg, 0).cuda()\n",
    "imshow(timg)\n",
    "\n",
    "print('Adversarial Example:')\n",
    "imshow(adv)\n",
    "\n",
    "def save_img(img, filename):\n",
    "    npimg = img[0].cpu().numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    pil_img = Image.fromarray(np.uint8(npimg * 255))\n",
    "    pil_img.save(filename)\n",
    "save_img(adv, 'adv.jpg')\n",
    "imshow(adv)\n",
    "\n",
    "print('Perturbation Heat Map:')\n",
    "perturbation_heat_map(oimg, adv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317a993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd6886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
